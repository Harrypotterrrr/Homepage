---
layout: article
title: "Conditional Generative Model"
aside:
  toc: true
sidebar:
  nav: Computer Vision
tags:
  - computer vision
  - generative model
  - notes
key: blog-2022-01-25-gan-2
date: 2022-01-25
modify_date: 2022-01-25
mathjax: true
---

Conditional Generative Model

<!--more-->

## Supervised Conditional

- Use paired data to train the network

### Conditional GAN

- Reason to add noise: Without noise, the network only learn the map from the input (e.g. text 'train', 'car', etc.) to the average of multiple positive samples (e.g. the average of the front and side train), which is very bad.

- Generator: take a normal (Gaussian) distribution and a constraint as inputs
- Discriminator: take generated output and a **constraint** as inputs, to check **both**:
  - if the result is realistic
  - if the result **matches** the constraint

- Algorithm:

1. Sample m examples $\\{ (c^1, x^{1}), (c^2, x^{2}), \ldots, (c^m, x^{m}) \\}$ from database
2. Sample m noise samples $\\{z^{1}, z^{2}, \ldots, z^{m}\\}$ from a distribution
3. Obtaining generated data $\\{\tilde{x}^{1}, \tilde{x}^{2}, \ldots, \tilde{x}^{m}\\}, \tilde{x}^{i}=G\left(z^{i}\right)$
4. Sample m objects $\\{\hat{x}^{1}, \hat{x}^{2}, \ldots, \hat{x}^{m}\\}$ from database
4. Update discriminator paramters $\theta_{d}$ to maximize  
 $$
  \begin{aligned}
  &\tilde{V}=\frac{1}{m} \sum_{i=1}^{m} \log D\left(c^{i}, x^{i}\right)+\frac{1}{m} \sum_{i=1}^{m} \log \left(1-D\left(c^{i}, \tilde{x}^{i}\right)\right)+\frac{1}{m} \sum_{i=1}^{m} \log \left(1-D\left(c^{i}, \hat{x}^{i}\right)\right) \\
  &\theta_{d} \leftarrow \theta_{d}+\eta \nabla \tilde{V}\left(\theta_{d}\right)
  \end{aligned}
 $$
5. Sample m noise samples $\\{z^{1}, z^{2}, \ldots, z^{m}\\}$ from a distribution
5. Sample m conditions $\\{c^{1}, c^{2}, \ldots, c^{m}\\}$ from a database
6. Update generator parameters $\theta_{g}$ to maximize  
 $$
 \tilde{V}=\frac{1}{m} \sum_{i=1}^{m} \log \left(D\left(G\left(c^{i}, z^{i}\right)\right)\right), \theta_{g} \leftarrow \theta_{g}-\eta \nabla \tilde{V}\left(\theta_{g}\right)
 $$

#### Discriminator

There are two commonly used prototype of structure of Discriminator:
1. Use two different network to receive object and condition. Green Network receive the object encoding and condition embedding and output final result which represent if generation is realistic and matched or not.
 ![gan-2-1](https://s1.ax1x.com/2022/04/13/LuDAPI.png){:style="margin:auto;display:block;"}
2. Two network output different scores. Blue network receives object encoding and condition to output match result.
 ![gan-2-2](https://s1.ax1x.com/2022/04/13/LuDEGt.png){:style="margin:auto;display:block;"}

- The first structure may confuse between the bad generation and bad matching.

#### Stack GAN

Progressively generate large resolution image by stacking generator.
- low resolution generator take embedding and noise to output small image
- The small images from the previous generator will be taken as input, replacing the noise, to refine the detail and enlarge the resolution.

 ![gan-2-3](https://s1.ax1x.com/2022/04/13/LuDZxf.png){:style="margin:auto;display:block;"}

#### Image-to-image

1. **Non-adversarial approach**: (**Traditional supervised approach using autoencoder**) directly mapping from the input to image
 ![gan-2-4](https://s1.ax1x.com/2022/04/13/LuDVRP.png){:style="margin:auto;display:block;"}
 The result is blurry because as stated in [Comparing to VAE](#comparing-to-vae), it is the average of several samples.
 ![gan-2-5](https://s1.ax1x.com/2022/04/13/LuDmM8.png){:style="margin:auto;display:block;"}
2. **Use GAN to generate**: generated image and conditions are fed into discriminator to judge realisitc or not. At the same time, *close constraint* is to guide generator to output as closed to the paired image in the training sample as possible.
 ![gan-2-6](https://s1.ax1x.com/2022/04/13/LuDnsS.png){:style="margin:auto;display:block;"}

- As the above figure shows, without *close constraint* part, which is implemented in *L1 distance* to measure the difference between results and paired real images, the output will be more diversed but something **unrelated** to the original input will be generated.

### PatchGAN

**TODO**

### Speech Enhancement

- More generally, the paired samples can be adopted to train GAN, e.g. noisy speech and clean voice pair.

![gan-2-7](https://s1.ax1x.com/2022/04/13/LuDuqg.png){:style="margin:auto;display:block;"}

### Video Generation

- Generator predict the future frame

![gan-2-8](https://s1.ax1x.com/2022/04/13/LuDMZQ.png){:style="margin:auto;display:block;"}


## Unsupervised Conditional

## Reference

- [Machine Learning And Having It Deep And Structured 2018 Spring, Hung-yi Lee](https://speech.ee.ntu.edu.tw/~hylee/mlds/2018-spring.php)
- [StackGAN: Text to Photo-realistic Image Synthesis with Stacked Generative Adversarial Networks](https://arxiv.org/abs/1612.03242)
- [Image-to-Image Translation with Conditional Adversarial Networks](https://arxiv.org/abs/1611.07004)
- [](https://arxiv.org/pdf/1611.07004.pdf)