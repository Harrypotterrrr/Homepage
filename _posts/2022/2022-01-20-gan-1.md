---
layout: article
title: "Generative Model"
aside:
  toc: true
sidebar:
  nav: Generative Model
tags:
  - computer vision
  - generative model
  - notes
key: blog-2022-01-20-gan-1
date: 2022-01-20
modify_date: 2022-01-20
mathjax: true
---

Deep Generative Model

<!--more-->

## Basic Idea of GAN

Each dimension of input vector represents some characteristic, which means that change the input of the noise, some of the characteristic of the output image will change.

The outout of discriminator is commonly a scalar. The larger value means realistic, while smaller means fake.

### Algorithm

Algorithm Initialize $\theta_{d}$ for $\mathrm{D}$, and $\theta_{g}$ for $\mathrm{G}$
- In each training iteration:
1. Sample m examples $\\{ x^{1}, x^{2}, \ldots, x^{m} \\}$ from database
2. Sample m noise samples $\\{z^{1}, z^{2}, \ldots, z^{m}\\}$ from a distribution
3. Obtaining generated data $\\{\tilde{x}^{1}, \tilde{x}^{2}, \ldots, \tilde{x}^{m}\\}, \tilde{x}^{i}=G\left(z^{i}\right)$
4. Update discriminator paramters $\theta_{d}$ to maximize  
 $$
\begin{aligned}
&\tilde{V}=\frac{1}{m} \sum_{i=1}^{m} \log D\left(x^{i}\right)+\frac{1}{m} \sum_{i=1}^{m} \log \left(1-D\left(\tilde{x}^{i}\right)\right) \\
&\theta_{d} \leftarrow \theta_{d}+\eta \nabla \tilde{V}\left(\theta_{d}\right)
\end{aligned}
 $$
5. Sample m noise samples $\\{z^{1}, z^{2}, \ldots, z^{m}\\}$ from a distribution
6. Update generator parameters $\theta_{g}$ to maximize  
 $$
\begin{aligned}
&\tilde{V}=\frac{1}{m} \sum_{i=1}^{m} \log \left(D\left(G\left(z^{i}\right)\right)\right) \\
&\theta_{g} \leftarrow \theta_{g}-\eta \nabla \tilde{V}\left(\theta_{g}\right)
\end{aligned}
 $$

- Takeaway:
  - Fix generator G, update discriminator D. Discriminator learns to assign high scores to real objects and low scores to generated result. Gradient **descent** of the final score.
  - Fix discriminator D, and update generator G. Gnerator learns to fool the discriminator. Gradient **ascent** of the final score.

### Generator

- Generator can be designed as **autoencoder**
  - Encoder part of the generator maps the original images to the latent space as the compact representation of the input object.
  - Decoder part of the generator reconstructs the original object from code to generated result as closely as possible.
  - After the training, we can **discard** the encoder part, replacing it with the input code as whatever we want.
  - Change the input vector of the input noise, the generated result will **vary smoothly**, but **may not make sense** e.g. linear interpolation of the input noise

![gan-1-1](https://s1.ax1x.com/2022/04/12/Le3aa4.png){:style="margin:auto;display:block;"}

**TODO** VAE

### Comparing to VAE

**TODO** TODO result in what?

- VAE: The relation between components are not-correlated since neurons in the same layer of the network will not influence each other and they are independent
- Thus VAE can not discriminate naunced between features, which cause the result more vague than GAN
- To catch the correpondence among pixels, deep structure needed.

![gan-1-2](https://s1.ax1x.com/2022/04/12/LeoGNT.png){:style="margin:auto;display:block;"}

## Discriminator

- Use discriminator to generate?
  - Inference: Generate object $\widetilde{x}$ that  
   $$
   \widetilde{x}=\arg \max _{x \in X} D(x)
   $$
  - Enumerate all possible $x$, which is not always feasible, because the above equations can only be solved when it is linear. However linearity constrain the ability of the network.

- Algorithm:
  - Given a set of positive examples and negative example generated randomly
  - In each iteration
    - Learn a discriminator D that can discriminate positive and negative examples
    - Find relatively good examples from negative examples by $\widetilde{x}=\arg \max _{x \in X} D(x)$
    - Replace the negative examples with relatively good examples

![gan-1-3](https://s1.ax1x.com/2022/04/12/Leo8EV.png){:style="margin:auto;display:block;"}

- From the above figure, we noticed that discriminator can only optimize the distribution corresponding to the generated distribution, which is one of the reason why GAN is difficult to optimize.

**TODO** structured learning

### GAN

- From Discriminator's point of view
  - Use Generator to generate negative samples, replacing infeasible sovled `argmax` function
- From Generator's point of view
  - Still generate the object **without correspondence** (pixel-by-pixel)
  - But it is led by the instruction of the discriminator with **global view**



manifold

- binary classifier : minimize cross entropy  而不是 minimize  mean square

- maximum object value is related to JS divergence

Generator: sampling from the manifold in high dimensional space

## Reference

- [Machine Learning And Having It Deep And Structured 2018 Spring, Hung-yi Lee](https://speech.ee.ntu.edu.tw/~hylee/mlds/2018-spring.php)