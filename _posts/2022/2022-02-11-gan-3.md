---
layout: article
title: "Advanced Thoery and Problems of GAN"
aside:
  toc: true
sidebar:
  nav: Generative Model
tags:
  - computer vision
  - generative model
  - notes
key: blog-2022-02-11-gan-3
date: 2022-02-11
modify_date: 2022-02-11
mathjax: true
---

Advanced Thoery and Problems of GAN

<!--more-->

## General Framework of GAN

### F-divergence

- $P$ and $Q$ are two distributions. $p(x)$ and $q(x)$ are the probability of sampling $x$.
- f-divergence $D_{f}(P \| Q)$ evaluates the difference of $P$ and $Q$.
  - $f$ is convex function
  - when $P$ and $Q$ are the same distributions, $p(x) = q(x)$, $f(\frac{p(x)}{q(x)}) = f(1) = 0$

$$
\begin{aligned}
D_{f}(P \| Q) &=\int_{x} q(x) f\left(\frac{p(x)}{q(x)}\right) d x \\
& \geq f\left(\int_{x} q(x) \frac{p(x)}{q(x)} d x\right) \color{red}{\text{Jessen Inequality}} \\
& = f\left(\int_{x}p(x) d x\right) \\
& = f(1) \\
& = 0
\end{aligned}
$$

**Note**: Considering $f$ is the convex function, the above inequation can be established according to [Jessen inequation](https://en.wikipedia.org/wiki/Jensen%27s_inequality):  
$ \varphi\left(\frac{1}{b-a} \int_{a}^{b} f(x) d x\right) \leq \frac{1}{b-a} \int_{a}^{b} \varphi(f(x)) d x $
{:.info}

- `f` function can be different

$$
\begin{aligned}
&\text{When } f(x)=x \log x \text{ , KL-divergence:}\\
&D_{f}(P \| Q)=\int_{x} q(x) \frac{p(x)}{q(x)} \log \left(\frac{p(x)}{q(x)}\right) d x=\int_{x} p(x) \log \left(\frac{p(x)}{q(x)}\right) d x \\[1em]
&\text{When }f(x)=-\log x \text{ , reversed KL-divergence:}\\
&D_{f}(P \| Q)=\int_{x} q(x)\left(-\log \left(\frac{p(x)}{q(x)}\right)\right) d x=\int_{x} q(x) \log \left(\frac{q(x)}{p(x)}\right) d x \\[1em]
&\text{When }f(x)=(x-1)^{2} \text{ , Chi Square-divergence:}\\
&D_{f}(P \| Q)=\int_{x} q(x)\left(\frac{p(x)}{q(x)}-1\right)^{2} d x=\int_{x} \frac{(p(x)-q(x))^{2}}{q(x)} d x
\end{aligned}
$$

### Fenchel Conjugate

- **Fenchel Conjugate** (Convex conjugate 凸共轭): the paired appearance of two functions with a strong relationship.
  - Duality (对偶) relationship is built on the same linear hyperplane.
- The **purpose** of using conjugate functions:
  - The advantage is that even if a function is not convex, **a convexed function** can be obtained by the conjugate method.
  - Even better, through the conjugation again, we can get a **good approximation** function with the original function.
  - The convex hull function of the original function can be obtained by conjugating **twice**, where many excellent properties of the new convex hull will be obtained in the optimization solution with just a little loss.

- Every convex function $f$ has a conjugate function $f^*$

$$
f^{*}(t) =\max _{x \in \operatorname{dom}(f)}\{x t-f(x)\}
$$

- The variable of $f^* (t)$ is $t$, and get the maximum for every possible value of $x$.

![gan-3-1](https://s1.ax1x.com/2022/04/18/LdZtRP.png){:style="margin:auto;display:block;width:80%;"}

- We can get a good approximation function with the original function
  - E.g. $f(x) = xlogx$ can be approximated as $f^*(t) = e^{t-1}$.

![gan-3-2](https://s1.ax1x.com/2022/04/18/LdZYGt.png){:style="margin:auto;display:block;width:60%;"}

$$
\begin{aligned}
&\text{Take } f(x) \text{ as } xlogx \text{ :}\\
&\quad f^{*}(t) =\max _{x \in \operatorname{dom}(f)}\{x t-f(x)\} = \max _{x \in \operatorname{dom}(f)}\{x t-x \log x\}\\
&\text{Let } g(x) =x t-x \log x \text {, Given } t, \text { find } x \text { maximizing } g(x) \text{:}\\
&\quad t-\log x-1=0 \quad \\ 
&\quad x= e^{t-1} \\
&\quad f^{*}(t)= e^{t-1} \times t-e^{t-1} \times(t-1)=e^{t-1} \\
\end{aligned}
$$

### Connection with GAN

- Derivation of GAN's derivation:
  - From the convex conjugate:  
   $$
   f^{*}(t)=\max _{x \in \operatorname{dom}(f)}\{x t-f(x)\} \longleftrightarrow f(x)=\max _{t \in \operatorname{dom}\left(f^{*}\right)}\left\{x t-f^{*}(t)\right\}
   $$
  - Take $x$ as $\frac{p(x)}{q(x)}$ :  
   $$
   \begin{aligned}
   D_{f}(P \| Q) &=\int_{x} q(x) f\left(\frac{p(x)}{q(x)}\right) d x \\
   &=\int_{x} q(x)\left(\max _{t \in \operatorname{dom}\left(f^{*}\right)}\left\{\frac{p  (x)}{q(x)} t-f^{*}(t)\right\}\right) d x \\
   &\color{red}{\geq} \int_{x} q(x)\left(\frac{p(x)}{q(x)} \color{red}{D(x)}-f^{*}( \color{red}{D(x)})\right) d x \\
   &=\int_{x} p(x) D(x) d x-\int_{x} q(x) f^{*}(D(x)) d x \\
   \end{aligned}
   $$

  - **The reason of** replacing $\max$ item with $D(x)$ as above in red color:
    - $D(x)$ takes $x$ as input and output $t$.
    - The limited capcity of $D$ determines the lower bound of $\max$ item.

  - For the divergence between $P$ and $Q$:  
  $$
   \begin{aligned}
   D_{f}(P \| Q) &\approx \max _{\mathrm{D}} \int_{x} p(x) D(x) d x-\int_{x} q(x) f^{*} (D(x)) d x \\
   &=\max _{\mathrm{D}}\left\{E_{x \sim P}[D(x)]-E_{x \sim Q}\left[f^{*}(D(x))\right] \right\} \\
   &\text { Samples from P } \quad \text { Samples from Q } \\
   \end{aligned}
  $$

  - Convert the conjugate to the more **general** form of GAN's objective by sampling from $P_{data}$ and $P_G$:
  $$
   D_{f}\left(P_{\text {data }} \| P_{G}\right)=\max _{\mathrm{D}}\left\{E_{x \sim P_{\text {data }}}[D(x)]-E_{x \sim P_{G}}\left[f^{*}(D(x))\right]\right\} \\
   \begin{aligned}
   G^{*} &=\arg \min _{G} D_{f}\left(P_{d a t a} \| P_{G}\right) \\
   &=\arg \min _{G} \max _{D}\left\{E_{x \sim P_{\text {data }}}[D(x)]-E_{x \sim P_{G}}\left[f^{*}(D(x))\right]\right\} \\
   &=\arg \min _{G} \max _{D} V(G, D)
   \end{aligned}
  $$
    - **Therefore**, according to the different **definition** of $V(G,D)$, we can have different **divergence**, which corresponds to different $f^*$ **function**.

![gan-3-3](https://s1.ax1x.com/2022/04/18/LdZaM8.png){:style="margin:auto;display:block;"}

<span class="ref">[*reference-2](#reference-2)</span>

  - **What benefit we can get to try different divergence** (and corresponding $f^*$ function)?

## Common problems

### Mode Collapseand & Dropping

- Mode collapse and Mode Dropping refer to **reduced variety** in the samples produced by a generator.
  - **Mode Collapse**: The generator synthesizes samples with **intra-mode** variety, but some modes are missing.
    - E.g. Generator outputs limited variaty of faces with many duplicates.
  - **Mode Dropping**: The generator synthesizes samples with **inter-mode** variety, but each mode lacks variety. 
    - E.g. Generator outputs various faces at once, but all with the same one feature, like white skins.

![gan-3-4](https://s1.ax1x.com/2022/04/18/LdZdsS.png){:style="margin:auto;display:block;width:60%;"}

- The problem may be caused by the bad selection of **divergence**

- As the below picture shows:
  - The left figure shows the problem of `KL-divergence`, where the distribution of the well-trained generator falls between the peak of the true data distribution. 
    - **This is also one explanation** of why traditional generation, like autoencoder, output obscure result by choosing KL-divergence and minimizing it (aka. maximizing the likelihood estimation).
    - The distribution between the peak refers to those obscure generated images.
  - The right figure shows the mode collapse and dropping problem, where the trained distribution mainly lies in one single peak.
    - This is one explanation of why advesarial training often encounters the limited diversity or limited features problems.
    - The missing distribution of generator referes to the missing features.

![gan-3-5](https://s1.ax1x.com/2022/04/18/LdZNxf.png){:style="margin:auto;display:block;width:70%;"}

- But choosing different divergence can not solve the problem completely.
  - One triky workarounds is train the different generator, each with limited diversity and mode collapse or dropping problems. Gather to be the better generator.


## Reference

1. [Machine Learning And Having It Deep And Structured 2018 Spring, Hung-yi Lee](https://speech.ee.ntu.edu.tw/~hylee/mlds/2018-spring.php)<a name="reference-1"></a>
2. [fGAN: General Framework of GAN](https://arxiv.org/abs/1606.00709)<a name="reference-2"></a>